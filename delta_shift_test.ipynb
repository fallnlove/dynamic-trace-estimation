{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse.linalg import expm, expm_multiply\n",
    "from pylanczos import PyLanczos\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем все алгоритмы из статьи [Dynamic trace estimation](https://arxiv.org/abs/2110.13752). Сначала все же начнем с вспомогательных функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RademacherVector(n: int):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            n: num of components\n",
    "        Output:\n",
    "            vec: random vector from Rademacher distribution\n",
    "    \"\"\"\n",
    "    assert n > 0\n",
    "\n",
    "    vec = np.random.binomial(1, 1/2, (n, 1))\n",
    "    vec[vec == 0] = -1\n",
    "\n",
    "    return vec\n",
    "\n",
    "def RademacherMatrix(shape):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            shape: shape of the matrix\n",
    "        Output:\n",
    "            mat: random matrix from Rademacher distribution\n",
    "    \"\"\"\n",
    "    mat = np.random.binomial(1, 1/2, shape)\n",
    "    mat[mat == 0] = -1\n",
    "\n",
    "    return mat\n",
    "\n",
    "def Hutchinson(oracle, n: int, l: int):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            oracle: oracle for implicit matrix-vector multiplication with A\n",
    "            n: size of the matrix A\n",
    "            l: number of iteration to approximate the trace of the matrix A\n",
    "        Output:\n",
    "            approximation: approximation to the trace of A\n",
    "    \"\"\"\n",
    "\n",
    "    assert l >= 0\n",
    "\n",
    "    approximation = 0\n",
    "    \n",
    "    for iter in range(l):\n",
    "        g = RademacherVector(n)\n",
    "        approximation += g.T @ oracle(g)\n",
    "    \n",
    "    return approximation[0, 0] / l\n",
    "\n",
    "def HutchinsonATA(oracle1, oracle2, n: int, l: int):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            oracle1: oracle for implicit matrix-vector multiplication with A\n",
    "            oracle2: oracle for implicit matrix-vector multiplication with B\n",
    "            n: size of the matrix A\n",
    "            l: number of iteration to approximate the trace of the matrix A.T @ B\n",
    "        Output:\n",
    "            approximation: approximation to the trace of the matrix A.T @ B\n",
    "    \"\"\"\n",
    "\n",
    "    assert l >= 0\n",
    "\n",
    "    approximation = 0\n",
    "    \n",
    "    for iter in range(l // 2):\n",
    "        g = RademacherVector(n)\n",
    "        approximation += (oracle1(g)).T @ oracle2(g)\n",
    "    \n",
    "    return approximation[0, 0] / l\n",
    "\n",
    "def SquaredFrobenius(oracle, n: int, l: int):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            oracle: oracle for implicit matrix-vector multiplication with A\n",
    "            n: size of the matrix A\n",
    "            l: number of iteration to approximate the frobenius norm of the matrix A\n",
    "        Output:\n",
    "            approximation: approximation to the frobenius norm of A\n",
    "    \"\"\"\n",
    "\n",
    "    assert l >= 0\n",
    "\n",
    "    approximation = 0\n",
    "    \n",
    "    for iter in range(l):\n",
    "        g = oracle(RademacherVector(n))\n",
    "        approximation += g.T @ g\n",
    "    \n",
    "    return approximation[0, 0] / l\n",
    "\n",
    "def HutchinsonPlusPlus(oracle, n: int, l: int):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            oracle: oracle for implicit matrix-vector multiplication with A\n",
    "            n: size of the matrix A\n",
    "            l: number of iteration to approximate the trace of the matrix A\n",
    "        Output:\n",
    "            approximation: approximation to the trace of A\n",
    "    \"\"\"\n",
    "\n",
    "    S = RademacherMatrix((n, l // 3))\n",
    "\n",
    "    Q, _ = np.linalg.qr(oracle(S))\n",
    "\n",
    "    return np.trace(Q.T @ oracle(Q)) + HutchinsonATA((lambda x: x - Q @ (Q.T @ x)), (lambda x: oracle(x - Q @ (Q.T @ x))), n, l - l // 3)\n",
    "\n",
    "def HutchinsonPlusPlusWithQ(oracle, n: int, l: int):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            oracle: oracle for implicit matrix-vector multiplication with A\n",
    "            n: size of the matrix A\n",
    "            l: number of iteration to approximate the trace of the matrix A\n",
    "        Output:\n",
    "            approximation: approximation to the trace of A\n",
    "            Q: matrix appended in the algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    S = RademacherMatrix((n, l // 3))\n",
    "\n",
    "    Q, _ = np.linalg.qr(oracle(S))\n",
    "\n",
    "    return np.trace(Q.T @ oracle(Q)) + HutchinsonATA((lambda x: x - Q @ (Q.T @ x)), (lambda x: oracle(x - Q @ (Q.T @ x))), n, l - l // 3), Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут напишем для сравнения алгоритмы, не использующие информацию о близости соседних матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleHutchinson(oracles, n: int, l: int):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            oracles: oracles for implicit matrix-vector multiplication with A1, ..., Am\n",
    "            n: matrix size\n",
    "            l: number of iteration to approximate the trace of the matrixes A1, ..., Am\n",
    "        Output:\n",
    "            approximation: list of approximations to the trace of A1, ..., Am\n",
    "    \"\"\"\n",
    "    assert len(oracles) > 0\n",
    "\n",
    "    approximation = []\n",
    "    \n",
    "    for i in range(len(oracles)):\n",
    "        approximation.append(Hutchinson(oracles[i], n, l))\n",
    "    \n",
    "    return approximation\n",
    "\n",
    "def SimpleHutchinsonPlusPlus(oracles, n: int, l: int):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            oracles: oracles for implicit matrix-vector multiplication with A1, ..., Am\n",
    "            n: matrix size\n",
    "            l: number of iteration to approximate the trace of the matrixes A1, ..., Am\n",
    "        Output:\n",
    "            approximation: list of approximations to the trace of A1, ..., Am\n",
    "    \"\"\"\n",
    "    assert len(oracles) > 0\n",
    "\n",
    "    approximation = []\n",
    "    \n",
    "    for i in range(len(oracles)):\n",
    "        approximation.append(HutchinsonPlusPlus(oracles[i], n, l))\n",
    "    \n",
    "    return approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем все возможные алгоритмы DeltaShift на основе обычного Хатчинсона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeltaShift(oracles, n: int, l0: int, l: int, gamma: float):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            oracles: oracles for implicit matrix-vector multiplication with A1, ..., Am\n",
    "            n: matrix size\n",
    "            l0: number of iteration to approximate the trace of the matrix A1\n",
    "            l: number of iteration to approximate the trace of the other matrixes\n",
    "            gamma: damping factor\n",
    "        Output:\n",
    "            approximation: list of approximations to the trace of A1, ..., Am\n",
    "    \"\"\"\n",
    "    assert len(oracles) > 0\n",
    "\n",
    "    approximation = []\n",
    "    approximation.append(Hutchinson(oracles[0], n, l0))\n",
    "    \n",
    "    for i in range(1, len(oracles)):\n",
    "        approximation.append((1 - gamma) * approximation[i - 1]\n",
    "                             +\n",
    "                             Hutchinson((lambda x: oracles[i](x) - (1 - gamma) * oracles[i - 1](x)), n, l))\n",
    "    \n",
    "    return approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParameterFreeDeltaShift(oracles, n: int, l0: int, l: int):\n",
    "    \"\"\"\n",
    "        Parameter free version of DeltaShift algorithm, gamma estimates inplace\n",
    "        \n",
    "        Input:\n",
    "            oracles: oracles for implicit matrix-vector multiplication with A1, ..., Am\n",
    "            n: matrix size\n",
    "            l0: number of iteration to approximate the trace of the matrix A1\n",
    "            l: number of iteration to approximate the trace of the other matrixes\n",
    "        Output:\n",
    "            approximation: list of approximations to the trace of A1, ..., Am\n",
    "    \"\"\"\n",
    "    assert len(oracles) > 0\n",
    "\n",
    "    approximation = [0]\n",
    "    N = 0\n",
    "\n",
    "    for _ in range(l0):\n",
    "        g = RademacherVector(n)\n",
    "        z = oracles[0](g)\n",
    "        approximation[0] += (g.T @ z)[0, 0]\n",
    "        N += (z.T @ z)[0, 0]\n",
    "\n",
    "    approximation[0] /= l0\n",
    "    N /= l0\n",
    "    variance = 2 / l0 * N\n",
    "\n",
    "    l //= 2\n",
    "    \n",
    "    for i in range(1, len(oracles)):\n",
    "        z = []\n",
    "        w = []\n",
    "        g = []\n",
    "        for j in range(l):\n",
    "            cur = RademacherVector(n)\n",
    "            z.append(oracles[i - 1](cur))\n",
    "            w.append(oracles[i](cur))\n",
    "            g.append(cur)\n",
    "\n",
    "        N = 0\n",
    "        M = 0\n",
    "        C = 0\n",
    "        for j in range(l):\n",
    "            N += (z[j].T @ z[j])[0, 0] / l\n",
    "            M += (w[j].T @ w[j])[0, 0] / l\n",
    "            C += (w[j].T @ z[j])[0, 0] / l\n",
    "\n",
    "        gamma = 1 - (2 * C) / (l * variance + 2 * N)\n",
    "\n",
    "        t = (1 - gamma) * approximation[i - 1]\n",
    "        for j in range(l):\n",
    "            t += (g[j].T @ (w[j] - (1 - gamma) * z[j]))[0, 0] / l\n",
    "\n",
    "        approximation.append(t)\n",
    "        variance = (1 - gamma)**2 * variance + 2 / l * (M + (1 - gamma)**2 * N - 2 * (1 - gamma) * C)\n",
    "    \n",
    "    return approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeltaShiftRestart(oracles, n: int, l0: int, l: int, q: int):\n",
    "    \"\"\"\n",
    "        DeltaShift algorithm, but restart every q iterations\n",
    "        Input:\n",
    "            oracles: oracles for implicit matrix-vector multiplication with A1, ..., Am\n",
    "            n: matrix size\n",
    "            l0: number of iteration to approximate the trace of the matrix A1\n",
    "            l: number of iteration to approximate the trace of the other matrixes\n",
    "            q: number of iterations to restart\n",
    "        Output:\n",
    "            approximation: list of approximations to the trace of A1, ..., Am\n",
    "    \"\"\"\n",
    "    assert len(oracles) > 0\n",
    "\n",
    "    approximation = []\n",
    "    \n",
    "    for i in range(len(oracles)):\n",
    "        if i % q == 0:\n",
    "            approximation.append(Hutchinson(oracles[i], n, l0))\n",
    "        else:\n",
    "            approximation.append(approximation[i - 1] +\n",
    "                                 Hutchinson((lambda x: oracles[i](x) - oracles[i - 1](x)), n, l))\n",
    "    \n",
    "    return approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем все возможные алгоритмы DeltaShift++ на основе обычного Хатчинсона++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeltaShiftPlusPlus(oracles, n: int, l0: int, l: int, gamma: float):\n",
    "    \"\"\"\n",
    "        Use Hutch++ instead of simple Hutchinson\n",
    "        Input:\n",
    "            oracles: oracles for implicit matrix-vector multiplication with A1, ..., Am\n",
    "            n: matrix size\n",
    "            l0: number of iteration to approximate the trace of the matrix A1\n",
    "            l: number of iteration to approximate the trace of the other matrixes\n",
    "            gamma: damping factor\n",
    "        Output:\n",
    "            approximation: list of approximations to the trace of A1, ..., Am\n",
    "    \"\"\"\n",
    "    assert len(oracles) > 0\n",
    "\n",
    "    approximation = []\n",
    "    approximation.append(HutchinsonPlusPlus(oracles[0], n, l0))\n",
    "    \n",
    "    for i in range(1, len(oracles)):\n",
    "        approximation.append(gamma * HutchinsonPlusPlus(oracles[i], n, l // 2)\n",
    "                             +\n",
    "                             (1 - gamma) *\n",
    "                             (approximation[i - 1] + HutchinsonPlusPlus((lambda x: oracles[i](x) - oracles[i - 1](x)), n, l // 2)))\n",
    "    \n",
    "    return approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeltaShiftPlusPlusRestart(oracles, n: int, l0: int, l: int, q: int):\n",
    "    \"\"\"\n",
    "        DeltaShift++ algorithm, but restart every q iterations\n",
    "        Input:\n",
    "            oracles: oracles for implicit matrix-vector multiplication with A1, ..., Am\n",
    "            n: matrix size\n",
    "            l0: number of iteration to approximate the trace of the matrix A1\n",
    "            l: number of iteration to approximate the trace of the other matrixes\n",
    "            q: number of iterations to restart\n",
    "        Output:\n",
    "            approximation: list of approximations to the trace of A1, ..., Am\n",
    "    \"\"\"\n",
    "    assert len(oracles) > 0\n",
    "\n",
    "    approximation = []\n",
    "    \n",
    "    for i in range(len(oracles)):\n",
    "        if i % q == 0:\n",
    "            approximation.append(HutchinsonPlusPlus(oracles[i], n, l0))\n",
    "        else:\n",
    "            approximation.append(approximation[i - 1] + HutchinsonPlusPlus((lambda x: oracles[i](x) - oracles[i - 1](x)), n, l // 2))\n",
    "    \n",
    "    return approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParameterFreeDeltaShiftPlusPlus(oracles, n: int, l0: int, l: int):\n",
    "    \"\"\"\n",
    "        Parameter free version of DeltaShift algorithm, gamma estimates inplace\n",
    "        \n",
    "        Input:\n",
    "            oracles: oracles for implicit matrix-vector multiplication with A1, ..., Am\n",
    "            n: matrix size\n",
    "            l0: number of iteration to approximate the trace of the matrix A1\n",
    "            l: number of iteration to approximate the trace of the other matrixes\n",
    "        Output:\n",
    "            approximation: list of approximations to the trace of A1, ..., Am\n",
    "    \"\"\"\n",
    "    assert len(oracles) > 0\n",
    "\n",
    "    approximation = []\n",
    "    appr, Q = HutchinsonPlusPlusWithQ(oracles[0], n, l0)\n",
    "    approximation.append(appr)\n",
    "\n",
    "    variance = 2 / l0 * SquaredFrobenius((lambda x: oracles[0](x) - Q @ (Q.T @ oracles[0](x))), n, l0)\n",
    "\n",
    "    \n",
    "    for i in range(1, len(oracles)):\n",
    "        appr1, Q1 = HutchinsonPlusPlusWithQ(oracles[i], n, l // 2)\n",
    "        appr2, Q2 = HutchinsonPlusPlusWithQ((lambda x: oracles[i](x) - oracles[i - 1](x)), n, l // 2)\n",
    "        K_A= SquaredFrobenius((lambda x: oracles[i](x) - Q1 @ (Q1.T @ oracles[i](x))), n, l // 2)\n",
    "        K_delta = SquaredFrobenius((lambda x:\n",
    "                                (oracles[i](x) - oracles[i - 1](x))\n",
    "                                - Q2 @ (Q2.T @ (oracles[i](x) - oracles[i - 1](x)))\n",
    "                                ), n, l // 2)\n",
    "\n",
    "        gamma = (8 * K_delta + l // 2 * variance) / (8 * K_A + l // 2 * variance + 8 * K_delta)\n",
    "\n",
    "        approximation.append(gamma * appr1\n",
    "                             +\n",
    "                             (1 - gamma) *\n",
    "                             (approximation[i - 1] + appr2))\n",
    "        variance = gamma**2 * 16 * K_A / l + (1  - gamma)**2 * (variance + 16 * K_delta / l)\n",
    "    \n",
    "    return approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к тестированию алгоритмов. Возьмем граф города Иннополис и посмотрим как он менялся со временем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckAlgorithms(oracles, n, correct_ans, title = None):\n",
    "    \"\"\"\n",
    "        Generate graphic with relative error\n",
    "        Input:\n",
    "            oracles: list of oracle to compute matrix-vector multiplication\n",
    "            n: matrix size\n",
    "            correct_ans: list of the trace of the given matrixes\n",
    "            title: title of the graphic\n",
    "        Output:\n",
    "            -\n",
    "    \"\"\"\n",
    "\n",
    "    l0 = 100\n",
    "    l = 50\n",
    "\n",
    "    \n",
    "    x = np.arange(len(oracles))\n",
    "    correct_ans = np.array(correct_ans)\n",
    "\n",
    "    simpl_hutchinson = np.array(SimpleHutchinson(oracles, n, l))\n",
    "    plt.plot(x, abs(correct_ans - simpl_hutchinson) / max(correct_ans), label=\"Hutchinson\")\n",
    "    simpl_hutchinsonpp = np.array(SimpleHutchinsonPlusPlus(oracles, n, l))\n",
    "    plt.plot(x,  abs(correct_ans - simpl_hutchinsonpp) / max(correct_ans), label=\"Hutchinson++\")\n",
    "\n",
    "    print(\"1\")\n",
    "\n",
    "    delta_shift = np.array(ParameterFreeDeltaShift(oracles, n, l0, l - 50 // 19))\n",
    "    plt.plot(x, abs(correct_ans - delta_shift) / max(correct_ans), label=\"DeltaShift\")\n",
    "    delta_shift_r = np.array(DeltaShiftRestart(oracles, n, l0, l - 50 // 19, 20))\n",
    "    plt.plot(x,  abs(correct_ans - delta_shift_r) / max(correct_ans), label=\"DeltaShift Restart\")\n",
    "\n",
    "    print(\"2\")\n",
    "\n",
    "    delta_shiftpp = np.array(ParameterFreeDeltaShiftPlusPlus(oracles, n, l0, l - 50 // 19))\n",
    "    plt.plot(x, abs(correct_ans - delta_shiftpp) / max(correct_ans), label=\"DeltaShift++\")\n",
    "    delta_shiftpp_r = np.array(DeltaShiftPlusPlusRestart(oracles, n, l0, l - 50 // 19, 20))\n",
    "    plt.plot(x,  abs(correct_ans - delta_shiftpp_r) / max(correct_ans), label=\"DeltaShift++ Restart\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time step(i)\")\n",
    "    plt.ylabel(\"$\\\\frac{|tr(A_i^3) - t_i|}{\\max_i tr(A_i^3)}$\")\n",
    "\n",
    "    if title != None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[987.1539785096313, 987.298315448448, 987.5692829348611, 987.5580724142576, 988.1985167713103, 986.8388245256483, 986.5726798713958, 986.2245726845852, 986.6973692335946, 985.6355406721506, 984.6405891204512, 983.5608013400863, 985.40444892322, 984.7850205301846, 984.1285644351038, 983.9632105760058, 984.3381662758758, 985.8186953112939, 985.6866961344067, 983.9527670513942, 984.3605190237101, 983.5996428764108, 983.1447904589936, 984.952076413195, 984.6234101270534, 985.215294386016, 985.6801091670517, 986.2901755640581, 986.4830723684111, 986.554583283631, 986.7757618940561, 987.698387504001, 987.853010106202, 987.6783797527366, 987.1463097468295, 986.2218350864389, 985.7487351944203, 986.2082296038964, 985.4078759266165, 984.1705154012529, 983.3225526717958, 983.9639058094808, 982.8374598304224, 981.982727250434, 983.0646238023911, 983.3864218703688, 983.3790084828588, 983.3489252853067, 983.7385591005634, 984.5281759820216, 983.5626228945044, 983.0483136134052, 982.8726625389191, 981.6516720892248, 981.9052134792677, 982.2985234987844, 982.0603514926104, 982.8605537168114, 981.8712783925445, 982.0158586383013, 983.5844981065839, 985.1279172301852, 985.5581468407336, 985.4283204431242, 984.6307451941789, 983.4545010109537, 983.2221311431861, 982.769933600397, 983.3006221839241, 981.9157519298492, 981.4884490254624, 979.3486027333489, 979.5220578873992, 981.9163846108452, 981.6841432702231, 981.8792099938918, 981.9287908831865, 981.6883955545413, 980.0833386626356, 978.7974661308556, 979.6436078027838, 981.179640313542, 981.4411164702594, 980.6285871950884, 979.5424571570102, 979.1865650719784, 979.4459840506705, 980.4697761263546, 981.1850835944704, 979.6631264884413, 980.7625803606768, 979.4640839617193, 981.1038516838498, 981.096683959426, 980.8413307673882, 980.8692047232205, 980.6559503164051, 980.7406690644882, 978.0707822312808, 977.9995826557694]\n"
     ]
    }
   ],
   "source": [
    "oracles = []\n",
    "correct_ans = []\n",
    "n = 100\n",
    "\n",
    "mat = np.random.random((2000, 2000))\n",
    "\n",
    "for i in range(n):\n",
    "    oracles.append((lambda x: mat @ x))\n",
    "    correct_ans.append(np.trace(mat))\n",
    "    mat += 5 * np.exp(-5) * RademacherVector(2000) @ np.random.random((1, 2000))\n",
    "\n",
    "print(correct_ans)\n",
    "\n",
    "# CheckAlgorithms(oracles, 2000, correct_ans, \"Very small perturbation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import datasets, transforms\n",
    "from data import * # get the dataset\n",
    "from pyhessian import hessian # Hessian computation\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# @file Different utility functions\n",
    "# Copyright (c) Zhewei Yao, Amir Gholami\n",
    "# All rights reserved.\n",
    "# This file is part of PyHessian library.\n",
    "#\n",
    "# PyHessian is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# PyHessian is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with PyHessian.  If not, see <http://www.gnu.org/licenses/>.\n",
    "#*\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def getData(name='cifar10', train_bs=128, test_bs=1000):\n",
    "    \"\"\"\n",
    "    Get the dataloader\n",
    "    \"\"\"\n",
    "    if name == 'cifar10':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        trainset = datasets.CIFAR10(root='../data',\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                                   batch_size=train_bs,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        testset = datasets.CIFAR10(root='../data',\n",
    "                                   train=False,\n",
    "                                   download=False,\n",
    "                                   transform=transform_test)\n",
    "        test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                                  batch_size=test_bs,\n",
    "                                                  shuffle=False)\n",
    "    if name == 'cifar10_without_dataaugmentation':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        trainset = datasets.CIFAR10(root='../data',\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                                   batch_size=train_bs,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        testset = datasets.CIFAR10(root='../data',\n",
    "                                   train=False,\n",
    "                                   download=False,\n",
    "                                   transform=transform_test)\n",
    "        test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                                  batch_size=test_bs,\n",
    "                                                  shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def test(model, test_loader, cuda=True):\n",
    "    \"\"\"\n",
    "    Get the test performance\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_num = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        pred = output.data.max(\n",
    "            1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "        total_num += len(data)\n",
    "    print('testing_correct: ', correct / total_num, '\\n')\n",
    "    return correct / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_vector_product(gradsH, params, v):\n",
    "    \"\"\"\n",
    "    compute the hessian vector product of Hv, where\n",
    "    gradsH is the gradient at the current point,\n",
    "    params is the corresponding variables,\n",
    "    v is the vector.\n",
    "    \"\"\"\n",
    "    hv = torch.autograd.grad(gradsH,\n",
    "                             params,\n",
    "                             grad_outputs=v,\n",
    "                             only_inputs=True,\n",
    "                             retain_graph=True)\n",
    "    return hv\n",
    "def group_product(xs, ys):\n",
    "    \"\"\"\n",
    "    the inner product of two lists of variables xs,ys\n",
    "    :param xs:\n",
    "    :param ys:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "546863.0625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m v \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandint_like(p, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m hessian_comp\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     19\u001b[0m ]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# b, _ = hessian_comp.eigenvalues()\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(b)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m Hv \u001b[38;5;241m=\u001b[39m \u001b[43mhessian_vector_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhessian_comp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradsH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessian_comp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(group_product(Hv, Hv)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mhessian_vector_product\u001b[0;34m(gradsH, params, v)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhessian_vector_product\u001b[39m(gradsH, params, v):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    compute the hessian vector product of Hv, where\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    gradsH is the gradient at the current point,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    params is the corresponding variables,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    v is the vector.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     hv \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradsH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                             \u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hv\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/__init__.py:412\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    409\u001b[0m         grad_outputs_\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    423\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    425\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get the model \n",
    "model = ptcv_get_model(\"preresnet20_cifar10\", pretrained=True)\n",
    "# change the model to eval mode to disable running stats upate\n",
    "model.eval()\n",
    "\n",
    "# create loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# get dataset \n",
    "train_loader, test_loader = getData()\n",
    "\n",
    "# for illustrate, we only use one batch to do the tutorial\n",
    "for inputs, targets in train_loader:\n",
    "    hessian_comp = hessian(model, criterion, data=(inputs, targets), cuda=False)\n",
    "    # a, b = hessian_comp.dataloader_hv_product(np.zeros(100))\n",
    "    v = [\n",
    "        torch.randint_like(p, high=2)\n",
    "        for p in hessian_comp.params\n",
    "    ]\n",
    "    # b, _ = hessian_comp.eigenvalues()\n",
    "    # print(b)\n",
    "    Hv = hessian_vector_product(hessian_comp.gradsH, hessian_comp.params, v)\n",
    "    print(group_product(Hv, Hv).cpu().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
